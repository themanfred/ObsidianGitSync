![[Pasted image 20230710192154.png]]

A [[Deep Learning (NN)]] algorithm
![[Pasted image 20230710192223.png]]


To do this we have [[Backproapagation]]

In this case a gradient over the entire dataset is too computationally complicated
![[Pasted image 20230710195215.png]]

Now we have only one gradient, and it is going to be very noicy. BUUUUT it will give us an appoximation.

But rather than doing one example, we would be doing 10s-100s of data points we have an increase **and have a more accurate estimation of gradient**.

**Have mini batches which lead to faster training.**


With this we have the Neural Networks in Practice - [[Overfitting]]


