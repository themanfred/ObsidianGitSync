
Generative pre-trained transformer (GPT) is an autoregressive 

A language model trained by OpenAI to predict the next word.
60% data for pre-training GPT-3 was taken from common crawl

It has been trained with 45 TB text data which includes sources from wikipedia and books.

It has 96 decode layers and is built on a system with 285k CPU cores, 10k GPU and 400 Gbps network for each GPU server.

![[Pasted image 20230410142818.png]]


### Dataset used to train gpt3

![[Pasted image 20230410142842.png]]



### Accuracy 

![[Pasted image 20230410143243.png]] 

The longer the context (larger the dataset assuming it is a good data test the better the accuaracy)